<!doctype html>
<html lang="en">
<head>
<title>Better Than ReLU?</title>
<meta property="og:title" content="Better Than ReLU?" />
<meta name="twitter:title" content="Better Than ReLU?" />
<meta name="description" content="Evaluating modern activation functions." />
<meta property="og:description" content="Evaluating modern activation functions." />
<meta name="twitter:description" content="Evaluating modern activation functions." />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" /> 
<meta name="viewport" content="width=device-width,initial-scale=1" />
<!-- bootstrap for mobile-friendly layout -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css" integrity="sha384-xOolHFLEh07PJGoPkLv1IbcEPTNtaed2xpHsD9ESMhqIYd0nLMwNLD69Npy4HI+N" crossorigin="anonymous">
<script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js" integrity="sha384-Fy6S3B9q64WdZWQUiU+q4/2Lc9npb8tCaSX9FK7E8HnRr0Jz8D6OP9dO5Vg3Q9ct" crossorigin="anonymous"></script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,700" rel="stylesheet">
<link href="style.css" rel="stylesheet">

</head>
<body class="nd-docs">
<div class="nd-pageheader">
 <div class="container">
 <h1 class="lead">
 <nobr class="widenobr">Better Than ReLU? Evaluating Modern Activation Functions</nobr>
 </h1>
 </div>
</div><!-- end nd-pageheader -->

<div class="container">
<div class="row">
<div class="col justify-content-center text-center">
<h2>An Analysis of <cite>Searching for Activation Functions</cite> (Ramachandran et al., 2017)</h2>
<div><b>Tim Wang, Ryan Dombroski</b></div>
<div>Northeastern University</div>
<div style="font-family: 'Courier New', Courier, monospace;">{wang.tim,dombroski.r}@northeastern.edu</div>
</div>
</div>
<div class="row">
<div class="col">

<h2>Introduction</h2>
<p>A key component of any neural network is a nonlinear activation function. Due to its simplicity and reliability, the current standard for nonlinear activation functions is the rectified linear unit, or ReLU (<a href="#hahnloser-2000">Hahnloser et al., 2000</a>; <a href="#jarrett-2009">Jarrett et al., 2009</a>; <a href="#nair-2010">Nair & Hinton, 2010</a>), defined as \(f(x) = \max(x, 0).\) Attempts have been made to construct variations on or alternatives to ReLU, but wide adoption of activation functions other than ReLU remains low.</p>

<p>Recent search algorithm based approaches have shown to produce novel activation functions (<a href="#zoph-2016">Zoph & Le, 2016</a>; <a href="#bello-2017">Bello et al., 2017</a>; <a href="#zoph-2017">Zoph et al., 2017</a>). A variation on the sigmoid linear unit, or SiLU (<a href="#hendrycks-2016">Hendrycks & Gimpel, 2016</a>) has been found to consistently outperform ReLU and other popular activation functions in accuracy metrics by up to one percent (<a href="ramachandran-2017">Ramachandran et al., 2017</a>) on various learning tasks, such as image classification and machine translation.</p>
  
<p>In this work we analyze and validate a number of activation functions discovered by search algorithm based approaches. In particular, we take a closer look at the SiLU function, comparing it and other novel activation functions to ReLU, as well as popular activation functions like Leaky ReLU (<a href="#maas-2013">Maas et al., 2013</a>), Parametric ReLU (<a href="#he-2015">He et al., 2015</a>), and Softplus (<a href="#nair-2010">Nair & Hinton, 2010</a>).</p>

<h2>Literature Review</h2>
<p>Ramachandran et. al. employ automated search techniques to discover and evaluate activation functions. This is done by designing a search space consisting of different compositions of unary and binary functions, and using an RNN controller to generate sequences of component functions to compose together. The candidate activation function was then used to train and then evaluate the performance of a child neural network on a learning task, in this case image classification on the CIFAR10 (<a href="#krizhevsky-2009">Krizhevsky & Hinton, 2009</a>) dataset using a ResNet-20 (<a href="#he-2016">He et al., 2016</a>) model.</p>

<p>Among a wide variety of novel activation functions discovered, the best performing functions:
  <ul>
    <li>are simple compositions of a small number of components,</li>
    <li>use the untransformed pre-activation as a component in a binary function; that is, have the form \(b(x, g(x)),\) and</li>
    <li>do not use division.</li>
  </ul>
</p>

<p>The strongest performing activation function discovered was a variation on SiLU, defined as \(f(x) = x \sigma(x).\) It is structurally very similar to ReLU, and able to directly replace ReLU without adjustments to the model architecture. Subsequent empirical validation experiments showed that SiLU outperformed a wide variety of both newly discovered activation functions, as well as popular activation functions presented in other papers.</p>

<h2>Methodology</h2>
<p>We reproduce selected experiments discussed by Ramachandran et al. to empirically validate the results presented. Several ResNet models were trained on an image classification task using the CIFAR10 dataset, varying the activation function used, as well as the depth of the model. Training loss, test loss, and test accuracy were recorded for each experiment, in addition to total training time. For all experiments we use the model with ReLU as a baseline, the Adam optimizer with PyTorch defaults, and cross entropy as the loss function.</p>

<h4>Choice of activation function</h4>
<p>From the novel activation functions discovered by the search algorithm used by Ramachandran et al., we select three to validate. These are SiLU, \(\max(x, \sigma(x)),\) and \(\cos x - x.\)</p>

<p>Additionally, from the other popular activation functions that are also presented, we select three more to validate. These are Leaky ReLU (LReLU), Parametric ReLU (PReLU), and Softplus.</p>

<h4>SiLU parameter</h4>
<p>Ramachandran et al. additionally propose a modification to SiLU that includes a tunable or learnable parameter \(\beta\), used to scale the preactivation value, resulting in the function \(f(x)=x \sigma(\beta x).\) We examine \(\beta\) values of 0.5, 1.0 (as regular SiLU), 1.5, and 2.0 using a ResNet-20 model, and additionally allow the network to learn an optimal \(\beta\) value.</p>

<h4>Model parameters</h4>
<p>We vary the depth of the model to explore how the performance of each activation function scales with the model. Specifically, we train and evaluate each activation function on ResNet models of 20, 32, 44, and 56 layers. Due to limited training resources and time, we were unable to validate on larger models.</p>

<h2>Experimental Findings</h2>
<p>All experimental code and analysis is available on <a href="https://github.com/timaeusx/ds4440-activation-functions">GitHub</a>.</p>

<h4>SiLU performance</h4>
<p>The results of our empirical validation broadly support the results presented by Ramachandran et al. In Resnet-20, ResNet-32, and ResNet-44, SiLU achieved the highest test accuracy after 10 epochs, in most cases outperforming the other activation functions. Moreover, we observe lower test loss compared to other activation functions, potentially indicating that SiLU generalizes better.</p>

<table border="1" class="dataframe, center">
  <thead>
    <tr style="text-align: right;">
      <th>Activation Function</th>
      <th>Accuracy</th>
      <th>Train Loss</th>
      <th>Test Loss</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>ReLU</td>
      <td>0.8047</td>
      <td>0.505950</td>
      <td>0.595131</td>
    </tr>
    <tr>
      <td>SiLU</td>
      <td>0.8143</td>
      <td>0.485753</td>
      <td>0.553022</td>
    </tr>
    <tr>
      <td>\(\cos x - x\)</td>
      <td>0.7959</td>
      <td>0.641438</td>
      <td>0.587057</td>
    </tr>
    <tr>
      <td>\(\max(x, \sigma(x))\)</td>
      <td>0.7955</td>
      <td>0.581675</td>
      <td>0.614394</td>
    </tr>
    <tr>
      <td>PReLU</td>
      <td>0.8085</td>
      <td>0.449701</td>
      <td>0.598796</td>
    </tr>
    <tr>
      <td>LReLU</td>
      <td>0.8033</td>
      <td>0.471236</td>
      <td>0.604913</td>
    </tr>
    <tr>
      <td>Softplus</td>
      <td>0.7758</td>
      <td>0.683996</td>
      <td>0.671707</td>
    </tr>
  </tbody>
  <caption>Table 1: Activation function metrics in ResNet-20. See the <a href="https://github.com/timaeusx/ds4440-activation-functions/blob/main/Analysis.ipynb">Analysis</a> notebook for all tables.</caption>
</table>

<p>We observe that the two activation functions discovered by the search algorithm, \(\cos x - x\)  and \(\max(x, \sigma(x)),\) underperformed in all experiments, regardless of model size. These results suggest that these functions make poor choices for activation functions.</p>

<h4>Performance over training epochs</h4>
<p>Speed of convergence is an important performance metric for any neural network. We visualize test accuracy, training loss, and test loss for each activation function over all training epochs. </p>

<img src="images/resnet-44-metrics/resnet-44-relu.png" width="100%">
<img src="images/resnet-44-metrics/resnet-44-nafs.png" width="100%">
<img src="images/resnet-44-metrics/resnet-44-relulikes.png" width="100%">

<p>Figure 1: SiLU test accuracy, training loss, and test loss compared to ReLU (above), discovered activation functions (center), and ReLU-like functions (below) over all training epochs, on ResNet-44.</p>

<p>SiLU consistently remains ahead over almost all metrics. In future work, with additional time and resources, it may be fruitful to investigate performance over additional epochs to analyze these trends further.</p>

<h4>Model training times</h4>
<p>Total training time is also an important metric to explore. Here we visualize differences in training time between the different activation functions for each ResNet model.</p>

<img src="images/runtime-metrics.png" width="100%">
<p>Figure 2: Comparison of total training times for all ResNet models and activation functions. Time is measured in seconds.</p>

<p>Interestingly, SiLU networks were consistently faster to train than their ReLU counterparts. In fact, SiLU consistently placed in the top two fastest activation functions in each experiment. This observation suggests that by replacing ReLU with SiLU in neural networks, higher accuracy can be achieved in less time.</p>

<p>ResNet models using \(\cos x - x\) and \(\max(x, \sigma(x)\) as activation functions consistently took the longest to train. This is likely because of the higher complexity of periodic and exponential functions, which require far more additions and multiplications to compute compared to ReLU and ReLU-like functions.</p>

<h4>Variation of SiLU parameter</h4>
<p>Our results suggest that varying \(\beta\) as a fixed or learnable parameter does not significantly influence test accuracy. While leaving \(\beta\) as a learnable parameter started with the highest accuracy, SiLU with fixed \(\beta = 2\) achieved the highest final test accuracy.</p>

<img src="images/betas.png" width="100%">
<p>Figure 3: Test accuracy and loss metrics for different values of \(\beta.\)</p>

<table border="1" class="dataframe, center">
  <thead>
    <tr style="text-align: right;">
      <th>Activation Function</th>
      <th>Accuracy</th>
      <th>Train Loss</th>
      <th>Test Loss</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Learnable \(\beta\) (lsilu)</td>
      <td>0.8226</td>
      <td>0.487016</td>
      <td>0.533784</td>
    </tr>
    <tr>
      <td>\(\beta = 0.5\) (silu0.5)</td>
      <td>0.8151</td>
      <td>0.564699</td>
      <td>0.535829</td>
    </tr>
    <tr>
      <td>\(\beta = 1.5\) (silu1.5)</td>
      <td>0.8308</td>
      <td>0.487803</td>
      <td>0.508667</td>
    </tr>
    <tr>
      <td>\(\beta = 2.0\) (silu2.0)</td>
      <td>0.8317</td>
      <td>0.477034</td>
      <td>0.517551</td>
    </tr>
  </tbody>
  <caption>Table 2: SiLU results for different values of \(\beta\).</caption>
</table>

<p>We conclude that in most cases it suffices to use SiLU with the default value of \(\beta = 1.\)</p>

<h2>Conclusions</h2>
<p>Our results broadly confirm the conclusions drawn by Ramachandran et al. SiLU consistently outperforms ReLU and other popular activation functions over a variety of contexts and metrics. Most notably, direct replacement of ReLU with SiLU in ResNet models of different sizes resulted in test accuracy improvements of almost two percent. This is significant for what is essentially a one line code change. </p>

<p>Future directions for investigation include extending training to more than 10 epochs, validating a wider variety of novel activation functions discovered by other search based algorithms, and exploring reasons for the decline in SiLU performance as model size increases. Furthermore, in concurrence with Ramachandran et al., we note that further gains are likely to be observed when networks are designed specifically with SiLU in mind.</p>

<h2>References</h2>
<p><a name="bello-2017" href="https://arxiv.org/abs/1709.07417">Irwan Bello, Barret Zoph, Vijay Vasudevan, and Quoc V Le. Neural optimizer search with reinforcement learning. In <cite>International Conference on Machine Learning</cite>, pp. 459-468, 2017.</a></p>

<p><a name="hahnloser-2000" href="https://pubmed.ncbi.nlm.nih.gov/10879535/">Richard HR Hahnloser, Rahul Sarpeshkar, Misha A Mahowald, Rodney J Douglas, and H Sebastian Seung. Digital selection and analogue amplification coexist in a cortex-inspired silicon circuit. <cite>Nature</cite>, 405(6789): 947-51, 2000.</a></p>

<p><a name="he-2015" href="https://arxiv.org/abs/1502.01852">Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In <cite>Proceedings of the IEEE international conference on computer vision</cite>, pp. 1026-1034, 2015.</a></p>

<p><a name="he-2016" href="https://arxiv.org/abs/1512.03385">Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In <cite>Proceedings of the IEEE conference on computer vision and pattern recognition</cite>, pp. 770-778, 2016.</a></p>

<p><a name="hendrycks-2016" href="https://arxiv.org/abs/1606.08415">Dan Hendrycks & Kevin Gimpel. Gaussian error linear units (gelus). <cite>arXiv preprint arXiv:1606.08415</cite>, 2016.</a></p>

<p><a name="krizhevsky-2009" href="https://www.semanticscholar.org/paper/Learning-Multiple-Layers-of-Features-from-Tiny-Krizhevsky/5d90f06bb70a0a3dced62413346235c02b1aa086">Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical report, Technical report, University of Toronto, 2009.</a></p>

<p><a name="jarrett-2009" href="https://ieeexplore.ieee.org/document/5459469">Kevin Jarrett, Koray Kavukcuoglu, Yann LeCun, et al. What is the best multi-stage architecture for object recognition? In <cite>2009 IEEE 12th International Conference on Computer Vision</cite>, 2009.</a></p>

<p><a name="maas-2013" href="https://www.semanticscholar.org/paper/Rectifier-Nonlinearities-Improve-Neural-Network-Maas/367f2c63a6f6a10b3b64b8729d601e69337ee3cc">Andrew L Maas, Awni Y Hannun, and Andrew Y Ng. Rectifier nonlinearities improve neural network acoustic models. In <cite>International Conference on Machine Learning</cite>, volume 30, 2013.</a></p>

<p><a name="nair-2010" href="https://dl.acm.org/doi/10.5555/3104322.3104425">Vinod Nair and Geoffrey E Hinton. Rectified linear units improve restricted boltzmann machines. In <cite>International Conference on Machine Learning</cite>, 2010.</a></p>

<p><a name="ramachandran-2017" href="https://arxiv.org/abs/1710.05941">Prajit Ramachandran, Barret Zoph, Quoc V. Le. Searching for Activation Functions. <cite>arXiv preprint arXiv:1710.05941</cite>, 2017.</a></p>

<p><a name="zoph-2016" href="https://arxiv.org/abs/1611.01578">Barret Zoph and Quoc V Le. Neural architecture search with reinforcement learning. In <cite>International Conference on Learning Representations</cite>, 2016.</a></p>

<p><a name="zoph-2017" href="https://arxiv.org/abs/1707.07012">Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V Le. Learning transferable architectures for scalable image recognition. <cite>arXiv preprint arXiv:1707.07012</cite>, 2017.</a></p>

<p><a name="a-martyn" href="https://github.com/a-martyn/resnet/tree/master">ResNet Github Implementation Adapted for Model Training. <cite>Github Resnet Implementation</cite>, 2019</a></p>

<p><a name="pytorch-2024" href="https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html">Pytorch Quickstart Tutorial for Model Training. <cite>Pytorch Tutorial</cite>, 2024</a></p>

<p><a name="pytorch2-2017" href="https://pytorch.org/vision/stable/generated/torchvision.datasets.FashionMNIST.html">Pytorch FashionMNIST for Model Training. <cite>Pytorch MNIST</cite>, 2017</a></p>

<p><a name="pytorch3-2017" href="https://pytorch.org/vision/0.17/generated/torchvision.datasets.CIFAR10.html">Pytorch CIFAR10 for Model Training. <cite>Pytorch CIFAR10</cite>, 2017</a></p>


 
 
  
</div><!--col-->
</div><!--row -->

 
  
</div><!--col-->
</div><!--row -->
</div> <!-- container -->

<footer class="nd-pagefooter">
  <div class="row">
    <div class="col-6 col-md text-center">
      <a href="https://ds4440.baulab.info/">About DS 4440</a>
    </div>
  </div>
</footer>

</body>
<script>
$(document).on('click', '.clickselect', function(ev) {
  var range = document.createRange();
  range.selectNodeContents(this);
  var sel = window.getSelection();
  sel.removeAllRanges();
  sel.addRange(range);
});
// Google analytics below.
window.dataLayer = window.dataLayer || [];
</script>
</html>
